import os
import requests
import pandas as pd
from bs4 import BeautifulSoup
from wordcloud import WordCloud
import matplotlib.pyplot as plt
import streamlit as st
from collections import Counter
from modules.DB import SupabaseDB
from urllib.parse import urlparse, parse_qs
import re
from io import BytesIO

class crawlingArticle:
    def __init__(self):
        self.db = SupabaseDB()

    def collect_article(self):
        # ë„¤ì´ë²„ ê²½ì œ ë‰´ìŠ¤ ì›¹ í˜ì´ì§€ íŒŒì‹±
        headers = {"User-Agent": "Mozilla/5.0"}
        base_url = 'https://finance.naver.com/news/mainnews.naver?page='

        url_set = set()
        for page in range(1, 10):
            page_url = base_url + str(page)

            response = requests.get(page_url, headers=headers)
            if response.status_code != 200:
                print(f"í˜ì´ì§€ ë¡œë”© ì‹¤íŒ¨: {page_url}")
                continue

            soup = BeautifulSoup(response.text, 'html.parser')
            for a_tag in soup.find_all('a', href=True):
                href = a_tag['href']

                if '/news/news_read.naver' in href:
                    parsed = urlparse(href)
                    params = parse_qs(parsed.query)
                    article_id = params.get('article_id', [None])[0]
                    office_id = params.get('office_id', [None])[0]

                    if article_id and office_id:
                        true_url = f"https://n.news.naver.com/article/{office_id}/{article_id}"
                        url_set.add(true_url)

        # ê¸°ì‚¬ íƒ€ì´í‹€, ë³¸ë¬¸ ì…€ë ‰í„°
        title_selector = "#title_area > span"
        main_selector = "#dic_area"

        # url ë¦¬ìŠ¤íŠ¸ì™€ ê¸°ì‚¬ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•  ë¦¬ìŠ¤íŠ¸
        url_lst = list(url_set)
        art_lst = []

        # ì „ì²´ ê²½ì œ ë‰´ìŠ¤ ê¸°ì‚¬ íŒŒì‹±
        for url in url_lst:
            html = requests.get(url, headers={
                "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/110.0.0.0 Safari/537.36"
            })
            soup = BeautifulSoup(html.text, "lxml")
            
            # ë‰´ìŠ¤ ì œëª© ìˆ˜ì§‘
            title = soup.select(title_selector)
            title_lst = [t.text for t in title]
            title_str = "".join(title_lst)
            
            # ë‰´ìŠ¤ ë³¸ë¬¸ ìˆ˜ì§‘
            main = soup.select(main_selector)
            main_lst = []
            for m in main:
                m_text = m.text
                m_text = m_text.strip()
                main_lst.append(m_text)
            main_str = "".join(main_lst)

            art_dic = {}
            art_dic["title"] = title_str
            art_dic["main"] = main_str
            art_dic["url"] = url

            art_lst.append(art_dic)

        article_df = pd.DataFrame(art_lst)
        return article_df

    def visualize_wordcloud(self):
        st.header("ğŸ”‘ ì˜¤ëŠ˜ì˜ ë‰´ìŠ¤ í‚¤ì›Œë“œ")
        font_path = os.path.join("assets", "NanumGothic-Bold.ttf")
        article_df = self.load_article()

        # ì‚¬ìš©ì ì…ë ¥: ìµœëŒ€ ë‹¨ì–´ ìˆ˜ ë° ìƒ‰ìƒ í…Œë§ˆ
        with st.expander("âš™ï¸ ì„¤ì •", expanded=False):
            max_words = st.slider("ë‹¨ì–´ ìˆ˜", 50, 200, 150, 10)
            colormap = st.selectbox("ìƒ‰ìƒ í…Œë§ˆ", ["viridis", "Dark2", "plasma", "twilight", "cividis"])

        # í…ìŠ¤íŠ¸ ë³‘í•©
        text = " ".join(article_df["title"])
        
        # ì •ê·œí‘œí˜„ì‹ìœ¼ë¡œ í•œê¸€ë§Œ ì¶”ì¶œ
        words = re.findall(r'[ê°€-í£a-zA-Z0-9%./]{2,}', text)  # í•œê¸€,ì˜ì–´,ìˆ«ìë§Œ

        # ë¶ˆìš©ì–´ ì œê±°
        stopwords = {"íˆ¬ì360", "ì¢…ëª©"}
        filtered_words = [word for word in words if word not in stopwords]

        # ë‹¨ì–´ ë¹ˆë„ ê³„ì‚°
        word_counts = Counter(filtered_words)

        # ì›Œë“œí´ë¼ìš°ë“œ ìƒì„±
        wordcloud = WordCloud(
            font_path=font_path,
            width=800,
            height=400,
            background_color="white",
            max_words=max_words,
            colormap=colormap
        ).generate_from_frequencies(word_counts)

        # Streamlit ì‹œê°í™”
        fig, ax = plt.subplots()
        ax.imshow(wordcloud, interpolation="bilinear")
        ax.axis("off")
        st.pyplot(fig)

        # ì›Œë“œí´ë¼ìš°ë“œ ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ë²„íŠ¼
        buf = BytesIO()
        fig.savefig(buf, format="png", bbox_inches="tight")
        st.download_button(
            label="ğŸ–¼ï¸ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ",
            data=buf.getvalue(),
            file_name="wordcloud.png",
            mime="image/png"
        )

    def save_article(self):
        """Supabaseì— ê¸°ì‚¬ ë°ì´í„° ì €ì¥"""
        article_df = self.collect_article()
        article_data = article_df.to_dict(orient="records")
        self.db.insert_article_data_json(article_data)
        print("ë‰´ìŠ¤ ê¸°ì‚¬ ë°ì´í„°ê°€ Supabaseì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")

    def load_article(self):
        """Supabaseì—ì„œ ê¸°ì‚¬ ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°"""
        data = self.db.get_article_data_today()
        if not data:
            print("Supabaseì— ì €ì¥ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. ìƒˆë¡œ ìˆ˜ì§‘í•©ë‹ˆë‹¤.")
            self.save_article()
            data = self.db.get_article_data_today()
        return pd.DataFrame(data)

    def get_article(self):
        return self.article_df

    def get_recommended_article(self, user_id, username):
        recommended_article = self.db.get_recommended_articles(user_id)

        st.header(f"ğŸ” {username}ë‹˜ì˜ ì¶”ì²œ ë‰´ìŠ¤")
        st.write("\n")
        for idx, item in enumerate(recommended_article, 1):
            with st.container():
                st.markdown(f"### {idx}. {item['title']}")
                st.markdown(f"ğŸ“Œ **ì¶”ì²œ ì´ìœ **: {item['reason']}")
                st.markdown(f"ğŸ“ **ë³¸ë¬¸ ìš”ì•½**: {item['summary']}")
                st.markdown(f"ğŸ”— [ê¸°ì‚¬ ë§í¬ ë³´ê¸°]({item['url']})", unsafe_allow_html=True)

        # st.write(recommended_article)